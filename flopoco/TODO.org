TODO file for FloPoCo
https://xkcd.com/844/
* Version 5
** install: 
*** wcpg: need f2c libblas-dev liblapack-dev
** cleanup of the pipeline framework
2 use cases from Kassel:
 - we want to call optimal bit-heap scheduling algorithms, which will not be ASAP.
- we want to generate optimal adder DAGs, also not ASAP.
In both case, we want to provide to these algorithms the schedule of all the inputs.
  Typical case of the bit heap of a large multiplier: it adds 
     - bits from its DSP blocks (arrive after 2 or 3 cycles) 
     - bits from the logic-based multipliers (arrive at cycle 0 after a small delay)
  Real-world bit heaps (e.g. sin/cos or exp or log) have even more complex, difficult to predict BH structures.
1 use case from Lyon: pipelined adders (should know the schedule of the inputs to 

We want a robust solution that works for these use cases.
Current version 5 (hereafter refered to as Matei's code) is not efficient (it reschedules all the time) and overengineered WRT to these use cases.
Only drawback of the solution proposed below WRT Matei's code: it requires explicit calls to schedule() in some situations.
I consider this a good thing, it gives control.

Hypotheses:
H0: schedule is always called on the top-level operator.
  Even an explicit call to schedule() in a sub-component will schedule its top-level
  Beware: Wrapper and TestBench should not be parent operators of the operators they wrap, so as not to modify the schedule. 
H1: default schedule will always be ASAP. 
  A call to schedule() does what it can, then stops.
H2: schedule() does not reschedule anything: if a signal is already scheduled, it is skipped.
H3: shared operators are exclusively sub-cycle LUT-like operators (use cases so far: compressors, LUT-based mults in IntMultiplier)
  They define (possibly explicitely) the delay(s) between their input and output, but need not be scheduled. 

Schedule is called implicitely after the constructor of the top-level operator.
It may be called explicity by some code, in particular bit heap compression.
This somehow constraints the order of writing operator constructor code, but it is OK.
 
The algo should be:

If a bit heap bh is involved, the constructor
1/ perform all the bh->addBit(),
2/ explicitely calls schedule(),  which is supposed to schedule all the inputs 
	(this constrains constructor code order)
3/ calls generateCompressorVHDL(), which we delegate to Kassel.
Kassel compute their optimal architecture + schedule, and add it to the VHDL stream already scheduled
so that (thanks to H2) it will remain (and not be rescheduled ASAP)

For Martin: 
- Before generateCompressorVHDL is called, we will have the lexicographic timing 
  (i.e. cycle + delay within a cycle) for all the bits that are input to the bit heap.
  We really want Martin's algos to manage that.
  
- Martin's algorithms compute cycles + delays. Two options to exploit this information:
    a/ ignore the cycles, just have each signal declared with a delay in the compressor trees, 
       and hope the ASAP scheduler re-computes cycles that will  match those computed by Martin
    b/ let Martin directly hack the cycles and delays into the DAG -- probably much more code.
	I would vote for a/, but as Martin also minimizes registers, we should go for b/ 
To discuss.

- The BitHeap should be simplified, all the timing information should be removed: 
   it is now in the Signals (once they have been scheduled).
  So the actual interface to provide to Martin is not yet fixed.

** TODOs for Florent:
The current framework won't work with a shared instance inputting inputs from different cycles:
- for unique instances the input synchronization is done inside the subcomponent.
This is (probably) the reason for the current FixSOPC fail.
To fix it:
- doApplySchedule has to insert a "_dn" on the actual input signal name
- the value of n is outputSignal->cycle - actual->cycle.
- We can't extract this info in the current framework
- but at the point where we want to insert the delay (about Operator.cpp line 2120) we have instanceName
- so all we need to add is a map(uniqueInstanceName, cycle) populated by schedule() or so. 

** Options for pipeline delays versus functional delays
First, I got rid completely of the integer cycle information stored on the edges,
(this removes all the "0" on the edges of the dep graph in flopoco.dot) 
but  predecessors and successors are still lists of pairs: one TODO cleanup here.
(also, the word "functional" appears a lot of times in Operator.cpp: check each of them)
*** Forcing pipeline registers
use cases: 
1- Wrapper (not important as it could be a functional register)
   warning, we want to be able to Wrap a combinatorial operator.
2- exploiting internal registers of DSP and BlockRAM at high-frequency
3- people who would want to manually build/control their pipeline, 
   but still want it to be correct by construction 
Cleanest solution in my current opinion:
 - add an attribute to Signal, in parallel to criticalPathContribution: int forcedPipelineRegister
 - set it with an inelegant method (Signal::forcePipelineRegister(int cycles=1)
   This method should be seldom used anyway
 - consume it within schedule() in parallel to criticalPathContribution: very easy.
 - all the rest should work untouched.
For use case 2, we also have the option to 
   add a criticalPathContribution that is n times the cycle latency,
   have schedule() advance the cycle n times in this case (there is a #if for that)
However I'm afraid it doesn't work for intermediate frequency: 
   in a DSP, either you pipeline using the 3 internal registers 
      and you get a latency of d \approx 1/500Mhz,
      or you do not pipeline at all and  the latency is 3d.
      You can probably get a latency of 2d 
      but there is no intermediate option with a latency of 2.5d.
So anyway you need some DSP-specific pipeline management, 
   which could as well use forcePipelineRegister()
*** Functional registers

Use cases:
- Z^-1  in FIR, IIR
- LongAcc and other accumulators
- state register in a state machine
What we want:
- the output of a functional register has its schedule set to (0,0) just as the inputs of a root Operator
- pipelining (insertion of pipeline registers) is disabled on a loop 
For instance, 
  in a FIR we may still pipeline the bit heap,  
  in an IIR or state machine, we should no longer.
I would not attempt to detect loops, but provide a method that disables pipelining 
(currently possible using setCombinatorial but it is Operator-wide)
this is to discuss

Options:
- just provide a set of specific Operators that are registers (with reset or not, etc)
  This is not as trivial as it seems, because instance() and/or schedule() must detect them
- (preferred) add one attribute bool isFunctionalRegisterOutput to Signal,
  and only access it through 
  Operator::functionalRegister(string registerOutputName, string registerInputName)
   (currently addRegisteredSignalCopy but it is not good)
  This attribute is
    - read by schedule() to reset the schedule (and also avoid it going into an infinite loop)
    - read at code generation time to make sure registerInputName is registered
Thing to study: how to manage reset (it should be a Signal like the others I think)

*** comments by Martin (before they get buried in the mail forever)

Forcing pipeline registers: I really like the idea to have "forcePipelineRegister" in Signal for cases you definitely want a stage at a certain place.
Functional registers: I would also vote for "isFunctionalRegisterOutput" in Signal.

Maybe it would be a good idea to extend declare() as a convenience function for this to have the possibility to declare a FF like

vhdl << "Q <= " << declareFunctionalReg("D") << endl;

Or as another optional argument for declare() like

string declare(string name, const int width, bool isbus=true, Signal::SignalReg sigReg=Signal::None, Signal::SignalType regType = Signal::wire);

where Signal::SignalReg could be Signal::None, Signal::FunctionalReg or Signal::ForcedPipelineReg?

Then we would have

vhdl << "Q <= " << declare("D",1,false,Signal::FunctionalReg) << endl;

to implement a FF.

I think this is more readable/transparent than calling

functionalRegister("Q","D");

** Plan for bringup
*** Shifter for basic pipeline: DONE
*** IntAdder for explicit call to schedule(): DONE
*** FPAdd for simple subcomponents : DONE
*** FPDiv for low complexity shared subcomponents DONE
*** FixRealKCM for simple bit heap DONE, 
*** FixSOPC DONE
*** FixFIR DONE
*** FixIIR for large bit heaps + functional delays
*** FixFunctionByTable (check that Table does the delay properly in the blockram case)
*** ALAP rescheduling for constant signals
*** IntMult DONE
*** FixSinCos for method=0

** Small things
*** call schedule() in instance()
*** constant signal scheduling
The constant signals are currently all scheduled to cycle 0.
This is stupid: once the schedule is done (all the outputs of the top-level are scheduled).
they should be rescheduled ALAP (time zero, cyle= min of the successors)

*** Signal::hasBeenScheduled_
Apart from that, I hope to remove most accesses to Signal::hasBeenScheduled_ that allow to re-schedule a signal:
   it should be initialized to false, then set once to true forever.

*** Re-check table attributes for 7 series; update Table, probably refactor this into Target 
*** In the dot output, package shared components into dotted boxes (subgraph cluster_)
... doesn't seem that simple
** List of operators to port to the new pipeline framework
	Remove an operator from the list below when it is done.
THIS INCLUDES WRITING ITS AUTOTEST

	IntConstMult: still need to 
	1/ add the delays 
	2/ resurrect IntConstMultRational (parseArgument etc)
	3 fix the mult by 7 in 2 additions 

	IntConstDiv: still need to
	1/ add the delays 
	2/ fix the bug that duplicates the tables

		Fix2FP
		FP2Fix
		InputIEEE
		OutputIEEE

		//FOR TEST PURPOSES ONLY
		Table

		IntComparator
		IntDualSub
		IntMultiplier
		IntSquarer

		FPConstMult
		FPRealKCM
		FPConstDiv
		FixFunctionByTable
		FixFunctionBySimplePoly
		FixFunctionByPiecewisePoly
		FixFunctionByMultipartiteTable
		BasicPolyApprox
		PiecewisePolyApprox
		FixRealKCM
		TestBench
		Wrapper
		FPAdd
		FPAddSub
		FPAddDualPath
		FPAdd3Input
		FPAddSinglePath
		FPMult
		//FPMultKaratsuba
		FPSquare
		FPDiv
		NbBitsMinRegisterFactory();
		FPSqrt

		FPLargeAcc
		LargeAccToFP
		FPDotProduct

		FPExp
		IterativeLog
		FPPow
		FixSinCos
		CordicSinCos
		FixAtan2
		//FixedComplexAdder

		FixRootRaisedCosine
		add FixSinc? 

		Complex/* (lots of cleanup)

		UserDefinedOperator
		
* Bugs
** constant 1000 bits in TestBench doesn't allow for parallel FFTs
** ./flopoco verbose=2 FixFunctionBySimplePoly plainvhdl=true f="sin(x)" lsbIn=-16 msbOut=4 lsbout=-16 TestBench n=-2
The parity problem leads to wrong alignment.
Nobody should do this if they have read the Muller book, so... people will try this and it is a bug
** ./flopoco verbose=2 FixFunctionBySimplePoly plainvhdl=true f="exp(x)" lsbIn=-16 msbOut=4 lsbout=-16 TestBench n=-2

** ./flopoco plainVHDL=1 FixFunctionByPiecewisePoly f="(2^x-1)" d=2 lsbIn=-1 lsbOut=-8 msbout=0 testbench
* Release-critical (current regressions):
** FPPipeline
** lut_rng
* Cleaning up
** replace target->isPipelined() (and getTarget->isPipelined()) with isSequential()
Rationale: the two are redundant. isSequential is less prone to change during the life of an Operator... 
isSequential is properly initialized out of isPipelined in the default Operator constructor.
DONE more or less in Operator
** Check that ?? and $$ and "port map" in comments don't ruin the pipeline framework
** get rid of rst signal
Observation: no operator uses rst, except FixFIR and LargeAcc. 
There is a good reason for that: it would prevent the inference of srl logic.

Now FixFir doesn't manage rst in emulate(), which is a framework limitation.
LargeAcc ignores rst. Instead it has an additional newDataSet input, which technically induces a synchronous reset
We should generalize this way of expressing reset information.
Benefit: it will remove rst from all the classical pipelined operators, and explicit it only when it is useful.

** get rid of use() in Operator
** Clean up DualTable
** Get rid of the useBitHeap arg in KCMs
** bug  ./flopoco FixSinCos -16 TestBenchFile 1000
  (close corresponding bug when fixed)
** change interface to FixSinCos and CordicSinCos to use lsb and not w
** IntConstMult: signed or unsigned int? (fix main.cpp)
** rounding bug:  ./flopoco FixRealKCM 1 3 -10 -10 "Pi" 1 TestBenchFile 1000
  (close corresponding bug when fixed)
** Check IntDualSub situation
** resurrect Guillaume's work (IntPower etc)
** Fuse CordicAtan2 and FixAtan2
** compression bug: ./flopoco IntMultiplier 2 16 16 1 0 0 does not produce a simple adder
** interface: simple and expert versions of IntMultiplier
** Here and there, fix VHDL style issues needed for whimsical simulators or synthesizers. See doc/VHDLStyle.txt
** For Kentaro: avoid generating multiple times the same operators. 
** Doxygenize while it's not too late

** clean up Target

* Targets
** Xilinx series 7
** Altera 10
* Continuous Integration
** autotest at commit
** set up a performance regression test as well
* Janitoring
*** replace inPortMap and outPortMap by the modern interface newInstance()
		See FPAddSinglePath for examples
*** build a SNAP package https://docs.snapcraft.io/build-snaps/  
*** Add modern targets
*** replace the big ifs in Target.cpp with  method overloading in subtargets ? 
*** gradually convert everything to standard lib arithmetic, getting rid of the synopsis ones.
*** TargetFactory
*** rename pow2, intpow2 etc as exp2
*** doxygen: exclude unplugged operators
*** See table attributes above
*** remove Operator::signalList, replace it with signalMap altogether
(this must be considered carefully, we have several lists)
*** Replace pointers with smart pointers ?
* Bit heap and multipliers
** rewrite BitHeap with fixed-point support and better compression (see Kumm papers and uni_kassel branch)
** pipeline virtual IntMult
** See UGLYHACK in IntMultiplier
** IntSquarer should be made non-xilinx-specific, and bitheapized
** Same for IntKaratsuba and FPKaratsuba, which have been disabled completely
** Get rid of SignedIO in BitHeap: this is a multiplier concern, not a bit heap concern
** get rid of Operator::useNumericStd_Signed etc
** get rid of bitHeap::setSignedIO(signedIO);
** Check all these registered etc nonsense in Signal. Is it really used?
** Bug (ds FixRealKCM?) ./flopoco -verbose=3 FPExp 7 12 
** With Matei: see the nextCycles in FPExp and see if we can push them in IntMultiplier somehow

* BitHeapization 
(and provide a bitheap-only constructor for all the following):
** systematic constructor interface with Signal variable
** FixSinCos
** Rework Guillaume Sergent's operators around the bit heap
** define a policy for enableSuperTile: default to false or true?
** Push this option to FPMult and other users of IntMult.
** Replace tiling exploration with cached/classical tilings
** More debogdanization: Get rid of
    IntAddition/IntCompressorTree
    IntAddition/NewIntCompressorTree
    IntAddition/PopCount
    after checking the new bit heap compression is at least as good...
** Check all the tests for "Virtex4"  src/IntAddSubCmp and replace them with tests for the corresponding features


Testbench

* Framework
** Bug on outputs that are bits with isBus false and  multiple-valued  
  (see the P output of Collision in release 2.1.0)
** Multiple valued outputs should always be intervals, shouldn't they?
** global switch to ieee standard signed and unsigned libraries
** fix the default getCycleFromSignal . 

* Wanted operators
** NormalCDF
... exists in the branch statistical_ops, old framework.
** FloatApprox
... exists in the random branch
** all in the random branch
** HOTBM
** Sum of n squares
** LUT-based integer comparators
** BoxMuller

* Improvements to do, operator by operator
** generic Hsiao compression could improve most table-based algorithms 
** Collision
*** manage infinities etc
*** decompose into FPSumOf3Squares and Collision

** HOTBM
*** true FloPoCoization, pipeline
*** better (DSP-aware) architectural exploration

ConstMult:

** ConstMult
*** group KCM and shift-and-add in a single OO hierearchy (selecting the one with less hardware)
*** For FPConstMult, don't output the LSBs of the IntConstMult 
   but only their sticky
*** more clever, Lefevre-inspired algorithm
*** Use DSP: find the most interesting constant fitting on 18 bits
*** compare with Spiral.net and Gustafsson papers
*** Implement the continued fraction stuff for FPCRConstMult
		
** Shifters
*** provide finer spec, see the TODOs inside Shifter.cpp

General

** FixSinCos 
*** currently does not use all the bit heaps that we advertize
*** see comments in FixSinCos.cpp One optim for 24 bits would be to compute z² for free by a table using the second unused port of the blockram

* If we could start pipeline from scratch MOSTLY DONE
If we were to redo the pipeline framework from scratch, here is the proper way to do it.

The current situation has a history: we first added cycle management, then, as a refinement, critical-path based subcycle timing.
So we have to manage explicitely the two components of a lexicographic time (cycle and delay within a cycle)
But there is only one wallclock time, and the decomposition of this wallclock time into cycles and sub-cycles could be automatic. And should.
 
The following version of declare() could remove the need for manageCriticalPath as well as all the explicit synchronization methods.
declare(name, size, delay)
declares a signal, and associates its computation delay to it.  This delay is what we currently pass to manageCriticalPath. Each signal now will have a delay associated to it (with a default of 0 for signals that do not add to the critical path).
The semantics is: this signal will not be assigned its value before the instant delta + max(instants of the RHS signals) 
This is all what the first pass, the one that populates the vhdl stream, needs to do. No explicit synchronization management needed. No need to setCycle to "come back in time", etc.

Then we have a retiming procedure that must associate a cycle to each signal. 
It will do both synchronization and cycle computation. According to Alain Darte there is an old retiming paper that shows that the retiming problem can be solved optimally in linear time for DAGs, which is not surprising.
Example of simple procedure: 
first build the DAG of signals (all it takes is the same RHS parsing, looking for signal names, as we do)
Then sit on the existing scheduling literature...
For instance  
1/ build the operator's critical path
2/ build the ASAP and ALAP instants for each signal
3/ progress from output to input, allocating a cycle to each signal, with ALAP scheduling (should minimize register count for compressing operators)
4/ possibly do a bit of Leiserson and Saxe retiming 

We keep all the current advantages: 
- still VHDL printing based
- When developing an operator, we initially leave all the deltas to zero to debug the combinatorial version. Then we incrementally add deltas, just like we currently  add manageCriticalPath(). 
- etc

The difference is that the semantic is now much clearer. No more notion of a block following a manageCriticalPath(), etc

The question is: don't we loose some control on the circuit with this approach, compared to what we currently do?

Note that all this is so much closer to textbook literature, with simple DAGs labelled by delay...

Questions and remarks:
- what to do with setPipeline depth? Currently, it is set by hand, but the new framework allows for it to be computed automatically from the cycles of the circuit's outputs. What to do when the outputs are not synchronized?
- should it be allowed to have delayed signals in a port map?
- should the constant signals be actual signals?
- how to handle instances:
  - we should create a new class Instance, which contains a reference to the instanced Operator and a portMap for its inputs and outputs
  - Operator should have a flag isGlobal
  - Instance should have a flag isImplemented, signaling if the operator is on the global operator list and whether it has already been implemented, or not
  - Operator has a list of the instances it creates
  - Operator has a list of sub-operators
  - Target has the global operator list
  - when creating a new instance of a global operator
    - if it is the first, then just add it to the  global operator list, with the isImplemented flag to true
    - if it is not the first, then clone the existing operator, connecting the clone's inputs/outputs to the right signals, and set the isImplemented flag to true
  - the global operators exist in Target as well, and will be implemented there
  - there should be no cycles in the graph
  - all architectures are unrolled in the signal graph
  
  !- resource estimation during timing: we already have some information about the circuit's interal, so why not use this information for resource estimation, as well?

* Options for signed/unsigned  DONE, text should stay here while the janitoring isn't done
Option 0: Do nothing radical. It seems when the options
 --ieee=standard --ieee=synopsys
are passed to ghdl in this order, we may mix standard and synopsys entities
See directory TestsSigned  
Incrementally move towards option 1 (for new operators, and when needed on legacy ones)

Option 1: 
 * Keep only std_logic_vector as IO,
 * Add an option to declare() for signed / unsigned / std_logic_vector DONE
    The default should still be std_logic_vector because we don't want to edit all the existing operators
 * add conversions to the VHDL. DONE 
 * No need to edit the TestBench architecture (DONE, actually some editing was needed)

Option 2 (out: see discussion below)
 Same as Option 1, but allow signed/unsigned IOs
 * Need to edit the TestBench architecture
 * Cleaner but adds more coding. For instance, in Table, need to manage the types of IOs.
 - Too many operators have sign-agnostic information, e.g. Table and all its descendants

---------------------------------------------------
Should we allow signed/unsigned IO?
- Good reason for yes: it seems to be better (cleaner etc)
- Good reason for no: many operators don't care (IntAdder, all the Tables) 
  and we don't want to add noise to their interface if it brings no new functionality.
- Bad reason for no: it is several man-days of redesign of the framework, especially TestBench
  Plus several man-weeks to manually upgrade all the existing operators
Winner: NO, we keep IOs as std_logic_vector.

Should the default lib be standard (currently synopsys)?
Good reason for yes: it is the way forward
Bad reasons for no:  it requires minor editing of all existing operators 
Winner: YES, but after the transition to sollya4 is complete and we have a satisfiying regression test framework.


